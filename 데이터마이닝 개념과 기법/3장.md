### 3장. 데이터 전처리

1. 개요
	- 데이터 품질은 데이터 사용의도에 따라 달라진다.
	- +사실성(사용자가 신뢰하는 정도), + 해석가능성(사용자가 이해하는 정도)

2. 데이터 정제 =클리닝
	- 결측치 처리 / 노이즈 조정 / 이상치 식별 및 제거 / 일관성
	1) 결측치
		+ 행(튜플) 무시 : 클래스 라벨이 없는 경우에 사용하는 방법. 그 외에 속성 열에 적용하는 것은 효과적인 방법은 아니다. 특히 속성별로 결측값의 비율이 다를 때 효과는 좋지 않음. -> ***차원의 저주 찾아보기***
		+ 수작업으로 채우기 : 현실적으로 어려움.
		+ 글로벌 상수값으로 채우기 : "Unknown"값으로 대체. 아예 무시하는 방법에 비해 "Unknown"값으로 대체할 경우, 유의미한 개념으로 이 값을 고려할 수도 있기 때문에 효과적일 수 있음.
		+ 중심 경향 측정값을 사용(평균, 중위수) : 대칭 데이터인 경우 평균, 편향된 데이터 분포인 경우 중위수를 사용. 
		+ 비슷한 튜플과 같은 데이터의 평균이나 중위수 이용 : 동일한 클래스에 속하는 튜플의 속성값의 평균이나 중위수를 사용.
		+ 가장 가능성이 높은 값으로 대체 : 회귀분석/베이지안 공식/의사결정나무 등을 이용하여 대체 값을 결정한다. 많이 사용하는 방법.
		+ 대체값의 경우 왜곡이 발생할 수 있으므로. 
	2) 노이즈 : random error
		+ 노이즈를 제거하기 위해 평활화smooth
		+ 비닝Binning : 데이터 값을 정렬하여 근접한 다른 값을 참고하여 평균이나 경계 등 평활화.
		+ 회귀분석 / 이상치분석(군집화하여 군집 외부에 있는 데이터를 이상치로) / moving average
		+ 'random으로 발생하고 데이터가 크면 처리하기가 현실적으로 어려울텐데', '학습 데이터가 크면 노이즈는 고려하지않아도 될까' 라는 생각이 들어서 찾아봤는데, [Deep Learning is Robust to Massive Label Noise](https://arxiv.org/pdf/1705.10694.pdf) 논문이 있었다.
		+ [요약하자면](https://github.com/jsj267/Reading-Material/blob/master/%EB%85%BC%EB%AC%B8/Deep%20Learning%20is%20Robust%20to%20Massive%20Label%20Noise.md)
		+ 머신러닝에서도 [노이즈가 절반까지는 학습이 잘 진행되나 80~90% 정도가 되면 어렵다고.](https://medium.com/@haho6629/noisy-data-8046056682ad)
		+ 노이즈 / 편향 / 이상치
			* 노이즈는 random error. 노이즈는 관심 대상 아니지만 이상치는 관심 대상. 이상치가 왜 발생했는지가 궁금함.
			* bias는 잘못된 가정을 하여 생기는 오차.
	3) 불일치
		+ 데이터에 대한 지식을 활용, 데이터 특성이나 기초 통계로 살펴보아 데이터의 일관성이 지켜졌는지, 데이터 표현이 정확한지 감지.

3. 데이터 통합
	- 데이터를 데이터웨어하우스에 저장하는 과정
	- 엔티티 확인, 속성 중복, 튜플 중복 등을 검사하고 확인해야함.

4. 데이터 축소
	- 집계, 중복제거, 군집화와 같은 과정을 통하여 데이터의 크기를 축소하는 과정.
	- 종류
		+ 차원 축소 : 웨이브렛 변환, 주성분 분석.
		+ 속성 부분 집합 선택 : 관련없는 속성 제거
		+ 숫자 축소 : 데이터량을 상대적으로 작은 표현 형태로 대체하는 방법. 회귀, 로그선형모형, 히스토그램, 클러스터링, 샘플링, 데이터 큐브 집계
		+ 데이터 압축 
	- 웨이브렛 변환
		+ 이산형 웨이브렛 변환 DWT; discrete wavelet transform
		+ 데이터 벡터 X에 선형신호처리 기술로 벡터X'로 변환한다.
	- 주성분분석PCA;principal component analysis
		+ 데이터를 잘 표현할 수 있는 직교벡터를 찾는 것. 즉, 데이터의 분산을 최대한 보존하려는 방법.
		+ 직교 변환(서로 직교하는 새 축에 데이터를 사상)을 통해 서로 연관 가능성이 있는 고차원 공간의 데이터를 선형 연관성이 없는 저차원 공간(주성분)의 데이터로 변환함. (그래서 연관성이 많은 데이터에 쓰면 효과가 좋다고 함. 추가로 노이즈가 많은 데이터에도 좋음.)
		+ 주성분분석의 차원수 < 원래 차원수 이므로 데이터를 축소시킬 수 있음.
		+ 데이터를 직교변환 시켰을 때 분산이 큰 순서대로 주성분을 정한다. 그리고 주성분 일부를 선택하기 때문에 분산을 최대한 보존할 수 있음.
		+ 큰 분산을 갖는다는 것은 해당 변수에 따라 데이터 값이 다양하게 된다는 것. 
		+ 주성분분석은 직교 변환하기 때문에 기존 변수를 선형결합해 새로운 변수를 만들고, 그 변수 중 일부를 사용하여 변수를 추출/선택한다.
	- 속성 부분 집합 선택
		+ 속성의 일부를 선택. 
		+ 그 선택 방법으로 전진선택법(forward) / 후향제거법(backward) / 단계적 선택법(stepwise) / 의사결정나무
		+ 전진선택법, 후향제거법, 단계적 선택법은 통계적 유의성 검증으로 각 속성의 좋고 나쁨을 결정하고, 의사결정나무 방법은 나무를 생성했을 때 중단 조건에 따라 속성 집합 결정. 
	- 데이터 축소 
		+ 모수 모형(회귀모형, 로그선형모형)을 이용하여 모형의 모수만 저장하여 축소에 사용
		+ 비모수 방법(히스토그램, 클러스터링, 샘플링)을 이용하여 데이터 축소
		+ 데이터 큐브 합계 : 관심의 정도가 더 요약된 단계일 경우, 데이터를 요약하면 데이터를 축소할 수 있음.

5. 데이터 변환 및 구분 : 데이터 마이닝에 적합한 형태로 변환하거나 통합
	- 평활화(노이즈 제거) : 비닝, 회귀, 클러스터링
	- 새로운 속성을 만들거나 추가
	- 집계(데이터 요약 또는 합산)
	- 정규화(범위 조정) : 큰 범위의 값은 작은 범위의 값에 비해 더 큰 가중치를 갖게 된다.
	- 숫자형 속성 계층 생성 : 간격 또는 개념 라벨을 만들어 계층에 포함시키기
	- 명목 데이터 계층 생성 : 개념 라벨을 만들어 계층에 포함시키기.
